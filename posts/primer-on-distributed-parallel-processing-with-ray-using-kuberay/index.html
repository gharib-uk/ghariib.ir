<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<style type=text/css>body{font-family:monospace;}</style>
	<title>&lt;div&gt;Primer on Distributed Parallel Processing with Ray using KubeRay&lt;/div&gt;</title>
	
	
	<link rel="stylesheet" href="/css/style.css">
	
	
</head>
<body>
	<header>
	==========================<br>
	== <a href="https://ghariib.ir/">Gharib Personal Blog</a> ==<br>
	==========================
	<div style="float: right;">A Techi Personal Blog</div><br>
	<p>
	<nav>
			<a href="/"><b>Start</b></a>.
			
			
			<a href="/posts/"><b>Posts</b></a>.
			
			<a href="/categories/"><b>Categories</b></a>.
			
			<a href="/tags/"><b>Tags</b></a>.
			
	</nav>
	</p>
	
</header>

	
	<main>
		<article>
			<h1>&lt;div&gt;Primer on Distributed Parallel Processing with Ray using KubeRay&lt;/div&gt;</h1>
			<b><time>03.01.2025 00:00</time></b>
		       

			<div>
				<p>In the early days of computing, applications handled tasks sequentially. As the scale grew with millions of users, this approach became impractical. Asynchronous processing allowed handling multiple tasks concurrently, but managing threads/processes on a single machine led to resource constraints and complexity.</p>
<p>This is where distributed parallel processing comes in. By spreading the workload across multiple machines, each dedicated to a portion of the task, it offers a scalable and efficient solution. If you have a function to process a large batch of files, you can divide the workload across multiple machines to process files concurrently instead of handling them sequentially on one machine. Additionally, it improves performance by leveraging combined resources and provides scalability and fault tolerance. As the demands increase, you can add more machines to increase available resources.</p>
<p>It is challenging to build and run distributed applications on scale, but there are several frameworks and tools to help you out. In this blog post, we’ll examine one such open source distributed computing framework: Ray. We’ll also look at KubeRay, a Kubernetes operator that enables seamless Ray integration with Kubernetes clusters for distributed computing in cloud native environments. But first, let’s understand where distributed parallelism helps.</p>
<h2 id="where-does-distributed-parallel-processing-help">Where does distributed parallel processing help?</h2>
<p>Any task that benefits from splitting its workload across multiple machines can utilize distributed parallel processing. This approach is particularly useful for scenarios such as web crawling, large-scale data analytics, machine learning model training, real-time stream processing, genomic data analysis, and video rendering. By distributing tasks across multiple nodes, distributed parallel processing significantly enhances performance, reduces processing time, and optimizes resource utilization, making it essential for applications that require high throughput and rapid data handling.</p>
<h3 id="when-distributed-parallel-processing-is-not-needed">When distributed parallel processing is not needed?</h3>
<ol>
<li><strong>Small-scale applications</strong>: For small datasets or applications with minimal processing requirements, the overhead of managing a distributed system may not be justified.</li>
<li><strong>Strong data dependencies</strong>: If tasks are highly interdependent and cannot be easily parallelized, distributed processing may offer little benefit.</li>
<li><strong>Real-time constraints</strong>: Some real-time applications (e.g., Finance and ticket booking websites) require extremely low latency, which might not be achievable with the added complexity of a distributed system.</li>
<li><strong>Limited resources</strong>: If the available infrastructure cannot support the overhead of a distributed system (e.g., insufficient network bandwidth, limited number of nodes), it may be better to optimize single-machine performance.</li>
</ol>
<h2 id="how-ray-helps-with-distributed-parallel-processing">How Ray helps with distributed parallel processing?</h2>
<p>Ray is a Distributed Parallel Processing framework that encapsulates all the benefits of distributed computing and solutions to challenges we discussed, such as fault tolerance, scalability, context management, communication, and so on. It is a pythonic framework, allowing the use of existing libraries and systems to work with it. With Ray’s help, a programmer doesn’t need to handle the pieces of the parallel processing compute layer. Ray will take care of scheduling and autoscaling based on the specified resource requirements.</p>
<p>
<figure>
  <img src="https://www.infracloud.io/assets/img/Blog/distributed-parallel-processing-ray-kuberay/ray.webp" alt="Ray" />
</figure>


</p>
<p>(Image Source: Ray provides a universal API of tasks, actors, and objects for building distributed applications.)</p>
<p>Ray provides a set of libraries built on the core primitives, i.e., Tasks, Actors, Objects, Drivers, and Jobs. These provide a versatile API to help build distributed applications. Let’s take a look at the core primitives, a.k.a. Ray Core.</p>
<h3 id="ray-core-primitives">Ray Core primitives</h3>
<ul>
<li><strong>Tasks</strong>: Ray tasks are arbitrary Python functions that are executed asynchronously on separate Python workers on a Ray cluster node. Users can specify their resource requirements in terms of CPUs, GPUs, and custom resources which are used by the cluster scheduler to distribute tasks for parallelized execution.</li>
<li><strong>Actors</strong>: What tasks are to functions, actors are to classes. An actor is a stateful worker, and the methods of an actor are scheduled on that specific worker and can access and mutate the state of that worker. Like tasks, actors support CPU, GPU, and custom resource requirements.</li>
<li><strong>Objects</strong>: In Ray, tasks and actors create and compute objects. These remote objects can be stored anywhere in a Ray cluster. Object References are used to refer to them, and they are cached in Ray’s distributed shared memory object store.</li>
<li><strong>Drivers</strong>: The program root, or the “main” program. This is the code that runs <code>ray.init()</code>.</li>
<li><strong>Jobs</strong>: The collection of tasks, objects, and actors originating (recursively) from the same driver and their runtime environment.</li>
</ul>
<p>For information about primitives, you can go through the Ray Core documentation.</p>
<h3 id="ray-core-key-methods">Ray Core key methods</h3>
<p>Below are some of the key methods within Ray Core that are commonly used:</p>
<ul>
<li>
<p><strong>ray.init()</strong> - Start Ray runtime and connect to the Ray cluster.</p>
<pre tabindex="0"><code>  import ray
  ray.init()
</code></pre></li>
<li>
<p><strong>@ray.remote</strong> - Decorator that specifies a Python function or class to be executed as a task (remote function) or actor (remote class) in a different process.</p>
<pre tabindex="0"><code>  @ray.remote
  def remote_function(x):
  	return x * 2
</code></pre></li>
<li>
<p><strong>.remote</strong> - Postfix to the remote functions and classes; remote operations are asynchronous.</p>
<pre tabindex="0"><code>  result_ref = remote_function.remote(10)
</code></pre></li>
<li>
<p><strong>ray.put()</strong> - Put an object in the in-memory object store; returns an object reference used to pass the object to any remote function or method call.</p>
<pre tabindex="0"><code>  data = [1, 2, 3, 4, 5]
  data_ref = ray.put(data)
</code></pre></li>
<li>
<p><strong>ray.get()</strong> - Get a remote object(s) from the object store by specifying the object reference(s).</p>
<pre tabindex="0"><code>  result = ray.get(result_ref)
  original_data = ray.get(data_ref)
</code></pre></li>
</ul>
<p>An example of using most of the basic key methods:</p>
<pre tabindex="0"><code>import ray

ray.init()

@ray.remote
def calculate_square(x):
	return x * x

# Using .remote to create a task
future = calculate_square.remote(5)

# Get the result
result = ray.get(future)
print(f&#34;The square of 5 is: {result}&#34;)
</code></pre><h3 id="how-does-ray-work">How does Ray work?</h3>
<p>Ray Cluster is like a team of computers that share the work of running a program. It consists of a head node and multiple worker nodes. The head node manages the cluster state and scheduling, while worker nodes execute tasks and manage actors.</p>
<p>
<figure>
  <img src="https://www.infracloud.io/assets/img/Blog/distributed-parallel-processing-ray-kuberay/ray-cluster.webp" alt="Ray Cluster" />
</figure>


</p>
<p>(A Ray cluster)</p>
<h4 id="ray-cluster-components">Ray Cluster components</h4>
<ul>
<li><strong>Global Control Store (GCS)</strong>: The GCS manages the metadata and global state of the Ray cluster. It tracks tasks, actors, and resource availability, ensuring that all nodes have a consistent view of the system.</li>
<li><strong>Scheduler</strong>: The scheduler distributes tasks and actors across available nodes. It ensures efficient resource utilization and load balancing by considering resource requirements and task dependencies.</li>
<li><strong>Head node</strong>: The head node orchestrates the entire Ray cluster. It runs the GCS, handles task scheduling, and monitors the health of worker nodes.</li>
<li><strong>Worker nodes</strong>: Worker nodes execute tasks and actors. They perform the actual computations and store objects in their local memory.</li>
<li><strong>Raylet</strong>: It manages shared resources on each node and is shared among all concurrently running jobs.</li>
</ul>
<p>You can check out the Ray v2 Architecture doc for more detailed information.</p>
<p>Working with existing Python applications doesn’t require a lot of changes. The changes required would mainly be around the function or class that needs to be distributed naturally. You can add a decorator and convert it into tasks or actors. Let’s see an example of this.</p>
<p><strong>Converting a Python function into Ray Task</strong></p>
<pre tabindex="0"><code># (Normal Python function)
def square(x):
	return x * x

# Usage
results = []
for i in range(4):
	result = square(i)
	results.append(result)
print(results)

# Output: [0, 1, 4, 9]

# (Ray Implementation)
# Define the square task.
@ray.remote
def square(x):
	return x * x

# Launch four parallel square tasks.
futures = [square.remote(i) for i in range(4)]
# Retrieve results.
print(ray.get(futures))
# -&gt; [0, 1, 4, 9]
</code></pre><p><strong>Converting a Python Class into Ray Actor</strong></p>
<pre tabindex="0"><code># (Regular Python class)
class Counter:
	def __init__(self):
    	self.i = 0
    
	def get(self):
    	return self.i
    
	def incr(self, value):
    	self.i += value

# Create an instance of the Counter class
c = Counter()

# Call the incr method on the instance
for _ in range(10):
	c.incr(1)

# Get the final state of the counter
print(c.get())  # Output: 10

# (Ray implementation in actor)
# Define the Counter actor.
@ray.remote
class Counter:
	def __init__(self):
    	self.i = 0
    
	def get(self):
    	return self.i
    
	def incr(self, value):
    	self.i += value

# Create a Counter actor.
c = Counter.remote()

# Submit calls to the actor. These
# calls run asynchronously but in
# submission order on the remote actor
# process.
for _ in range(10):
	c.incr.remote(1)

# Retrieve final actor state.
print(ray.get(c.get.remote()))
# -&gt; 10
</code></pre><p><strong>Storing information in Ray Objects</strong></p>
<pre tabindex="0"><code>
import numpy as np

# (Regular Python function)
# Define a function that sums the values in a matrix
def sum_matrix(matrix):
	return np.sum(matrix)

# Call the function with a literal argument value
print(sum_matrix(np.ones((100, 100))))  # Output: 10000.0

# Create a large array
matrix = np.ones((1000, 1000))

# Call the function with the large array
print(sum_matrix(matrix))  # Output: 1000000.0

# (Ray implementation of function)
import numpy as np

# Define a task that sums the values in a matrix.
@ray.remote
def sum_matrix(matrix):
	return np.sum(matrix)

# Call the task with a literal argument value.
print(ray.get(sum_matrix.remote(np.ones((100, 100)))))
# -&gt; 10000.0

# Put a large array into the object store.
matrix_ref = ray.put(np.ones((1000, 1000)))

# Call the task with the object reference as argument.
print(ray.get(sum_matrix.remote(matrix_ref)))
# -&gt; 1000000.0
</code></pre><p>To learn more about its concept, head over to Ray Core Key Concept docs.</p>
<h2 id="ray-vs-traditional-approach-of-distributed-parallel-processing">Ray vs traditional approach of distributed parallel processing</h2>
<p>Below is a comparative analysis between the Traditional (without Ray) Approach vs Ray on Kubernetes to enable distributed parallel processing.</p>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Traditional Approach</th>
          <th>Ray on Kubernetes</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Deployment</strong></td>
          <td>Manual setup and configuration</td>
          <td>Automated with KubeRay Operator</td>
      </tr>
      <tr>
          <td><strong>Scaling</strong></td>
          <td>Manual scaling</td>
          <td>Automatic scaling with RayAutoScaler and Kubernetes</td>
      </tr>
      <tr>
          <td><strong>Fault Tolerance</strong></td>
          <td>Custom fault tolerance mechanisms</td>
          <td>Built-in fault tolerance with Kubernetes and Ray</td>
      </tr>
      <tr>
          <td><strong>Resource Management</strong></td>
          <td>Manual resource allocation</td>
          <td>Automated resource allocation and management</td>
      </tr>
      <tr>
          <td><strong>Load Balancing</strong></td>
          <td>Custom load balancing solutions</td>
          <td>Built-in load balancing with Kubernetes</td>
      </tr>
      <tr>
          <td><strong>Dependency Management</strong></td>
          <td>Manual dependency installation</td>
          <td>Consistent environment with Docker containers</td>
      </tr>
      <tr>
          <td><strong>Cluster Coordination</strong></td>
          <td>Complex and manual</td>
          <td>Simplified with Kubernetes service discovery and coordination</td>
      </tr>
      <tr>
          <td><strong>Development Overhead</strong></td>
          <td>High, with custom solutions needed</td>
          <td>Reduced, with Ray and Kubernetes handling many aspects</td>
      </tr>
      <tr>
          <td><strong>Flexibility</strong></td>
          <td>Limited adaptability to changing workloads</td>
          <td>High flexibility with dynamic scaling and resource allocation</td>
      </tr>
  </tbody>
</table>
<p>Kubernetes provides an ideal platform for running distributed applications like Ray due to its robust orchestration capabilities. Below are the key pointers that set the value on running Ray on Kubernetes –</p>
<ul>
<li>Resource management</li>
<li>Scalability</li>
<li>Orchestration</li>
<li>Integration with ecosystem</li>
<li>Easy deployment and management</li>
</ul>
<p>KubeRay Operator to make it possible to run Ray on Kubernetes.</p>
<h2 id="what-is-kuberay">What is KubeRay?</h2>
<p>The KubeRay Operator simplifies managing Ray clusters on Kubernetes by automating tasks such as deployment, scaling, and maintenance. It uses Kubernetes Custom Resource Definitions (CRDs) to manage Ray-specific resources.</p>
<h4 id="kuberay-crds">KubeRay CRDs</h4>
<p>It has three distinct CRDs.</p>
<p>
<figure>
  <img src="https://www.infracloud.io/assets/img/Blog/distributed-parallel-processing-ray-kuberay/kuberay-crd.webp" alt="KubeRay CRDs" />
</figure>


</p>
<p>(Image source)</p>
<ul>
<li><strong>RayCluster</strong>: This CRD helps manage RayCluster’s lifecycle and takes care of AutoScaling based on the configuration defined.</li>
<li><strong>RayJob</strong>: It is useful when there is a one-time job you want to run instead of keeping a standby RayCluster running all the time. It creates a RayCluster and submits the job when ready. Once the job is done, it deletes the RayCluster. This helps in automatically recycling the RayCluster.</li>
<li><strong>RayService</strong>: This also creates a RayCluster but deploys a RayServe application on it. This CRD makes it possible to do in-place updates to the application, providing zero-downtime upgrades and updates to ensure the high-availability of the application.</li>
</ul>
<h2 id="use-cases-of-kuberay">Use-cases of KubeRay</h2>
<h3 id="deploying-an-on-demand-model-using-rayservice">Deploying an on-demand model using RayService</h3>
<p>RayService allows you to deploy models on-demand in a Kubernetes environment. This can be particularly useful for applications like image generation or text extraction, where models are deployed only when needed.</p>
<p>Here is an example of Stable Diffuison. Once it is applied in Kubernetes, it will create RayCluster and also run a RayService, which will serve the model until you delete this resource. It allows users to take control of resources.</p>
<h3 id="training-a-model-on-a-gpu-cluster-using-rayjob">Training a model on a GPU cluster using RayJob</h3>
<p>RayService serves different requirements to the user, where it keeps the model or application deployed until it is deleted manually. In contrast, RayJob allows one-time jobs for use cases like training a model, preprocessing data, or inference for a fixed number of given prompts.</p>
<h3 id="run-inference-server-on-kubernetes-using-rayservice-or-rayjob">Run inference server on Kubernetes using RayService or RayJob</h3>
<p>Generally, we run our application in Deployments, which maintains the rolling updates without downtime. Similarly, in KubeRay, this can be achieved using RayService, which deploys the model or application and handles the rolling updates.</p>
<p>However, there could be cases where you just want to do batch inference instead of running the inference servers or applications for a long time. This is where you can leverage RayJob, which is similar to the Kubernetes Job resource.</p>
<p>Image Classification Batch Inference with Huggingface Vision Transformer is an example of RayJob, which does Batch Inferencing.</p>
<p>These are the use cases of KubeRay, enabling you to do more with the Kubernetes cluster. With the help of KubeRay, you can run mixed workloads on the same Kubernetes cluster and offload GPU-based workload scheduling to Ray.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Distributed parallel processing offers a scalable solution for handling large-scale, resource-intensive tasks. Ray simplifies the complexities of building distributed applications, while KubeRay integrates Ray with Kubernetes for seamless deployment and scaling. This combination enhances performance, scalability, and fault tolerance, making it ideal for web crawling, data analytics, and machine learning tasks. By leveraging Ray and KubeRay, you can efficiently manage distributed computing, meeting the demands of today’s data-driven world with ease.</p>
<p>Not only that, but as our compute resource types are changing from CPU to GPU-based, it becomes important to have efficient and scalable cloud infrastructure for all sorts of applications, whether it be AI or large data processing. For that, you can bring in AI and GPU Cloud experts onboard to help you out.</p>
<p>We hope you found this post informative and engaging. For more posts like this one, subscribe to our weekly newsletter. We’d love to hear your thoughts on this post, so do start a conversation on LinkedIn :)</p>
<p>Go to Source</p>

			</div>
		</article>
	</main>
<aside>
	<div>
		<div>
			<h3>LATEST POSTS</h3>
		</div>
		<div>
			<ul>
				
				<li><a href="/posts/a-visual-summary-of-sans-new2cyber-summit-2025/">A Visual Summary of SANS New2Cyber Summit 2025</a></li>
				
				<li><a href="/posts/advance-your-cybersecurity-career-how-to-use-tuition-reimbursement-for-sec406/">Advance Your Cybersecurity Career: How to Use Tuition Reimbursement for SEC406</a></li>
				
				<li><a href="/posts/continuous-penetration-testing-a-consultants-perspective/">Continuous Penetration Testing - A Consultant’s Perspective</a></li>
				
				<li><a href="/posts/desktop-4-39-smarter-ai-agent-docker-desktop-cli-in-ga-and-effortless-multi-platform-builds/">Desktop 4.39: Smarter AI Agent, Docker Desktop CLI in GA, and Effortless Multi-Platform Builds</a></li>
				
				<li><a href="/posts/docker-engine-v28-hardening-container-networking-by-default/">Docker Engine v28: Hardening Container Networking by Default</a></li>
				
			</ul>
		</div>
	</div>
</aside>


	<footer>
	<p>&copy; 2025 <a href="https://ghariib.ir/"><b>Alireza Gharib. All right reserved</b></a>.
	<a href="https://github.com/Gharib110"><b>Github</b></a>.
	</p>
</footer>

</body>
</html>
