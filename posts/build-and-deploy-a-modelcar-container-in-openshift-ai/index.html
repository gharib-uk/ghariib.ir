<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<style type=text/css>body{font-family:monospace;}</style>
	<title>Build and deploy a ModelCar container in OpenShift AI</title>
	
	
	<link rel="stylesheet" href="/css/style.css">
	
	
</head>
<body>
	<header>
	==========================<br>
	== <a href="https://ghariib.ir/">Gharib Personal Blog</a> ==<br>
	==========================
	<div style="float: right;">A Techi Personal Blog</div><br>
	<p>
	<nav>
			<a href="/"><b>Start</b></a>.
			
			
			<a href="/posts/"><b>Posts</b></a>.
			
			<a href="/categories/"><b>Categories</b></a>.
			
			<a href="/tags/"><b>Tags</b></a>.
			
	</nav>
	</p>
	
</header>

	
	<main>
		<article>
			<h1>Build and deploy a ModelCar container in OpenShift AI</h1>
			<b><time>01.02.2025 00:00</time></b>
		       

			<div>
				<p>One challenge of managing models in production environments with Red Hat OpenShift AI and KServe is the dependency on S3-compatible storage. When serving models using KServe in OpenShift AI, users must deploy S3-compatible storage somewhere accessible in their cluster and then upload their model to an S3 bucket. Managing models through S3 creates new challenges for traditional operations teams deploying production services. Teams may need to create new processes to promote model artifacts from development/test environments.</p>
<p>OpenShift AI 2.14 enabled the ability to serve models directly from a container using KServe&rsquo;s ModelCar capabilities, and OpenShift AI 2.16 has added the ability to deploy a ModelCar image from the OpenShift AI dashboard. This article demonstrates how to build and deploy models with ModelCar containers in OpenShift AI and discusses the pros and cons.</p>
<h2 id="how-to-build-a-modelcar-container">How to build a ModelCar container</h2>
<p>ModelCar container requirements are quite simple; model files must be located in a <code>/models</code> folder of the container. The container requires no additional packages or files.</p>
<p>Developers can simply copy the model files from their local machine into the container. However, in our example we will use a two-stage build process to download a model from HuggingFace and then copy the files into a clean container.</p>
<p>We will use the Granite 3.1 2b Instruct model in our example.</p>
<h3 id="step-1-create-a-file">Step 1: Create a file</h3>
<p>Begin by creating a file called <code>download_model.py</code> with a simple script that uses the <code>huggingface-hub</code> package to download the model as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> huggingface_hub <span style="color:#f92672">import</span> snapshot_download
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Specify the Hugging Face repository containing the model</span>
</span></span><span style="display:flex;"><span>model_repo <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;ibm-granite/granite-3.1-2b-instruct&#34;</span>
</span></span><span style="display:flex;"><span>snapshot_download(
</span></span><span style="display:flex;"><span>    repo_id<span style="color:#f92672">=</span>model_repo,
</span></span><span style="display:flex;"><span>    local_dir<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;/models&#34;</span>,
</span></span><span style="display:flex;"><span>    allow_patterns<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;*.safetensors&#34;</span>, <span style="color:#e6db74">&#34;*.json&#34;</span>, <span style="color:#e6db74">&#34;*.txt&#34;</span>],
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>The script uses the <code>snapshot_download</code> function to download the Granite model to the <code>/models</code> folder.</p>
<h3 id="step-2-the-two-stage-build">Step 2: The two-stage build</h3>
<p>This two-stage build process allows us to minimize the content in the final container used in our deployment. While the difference in the container size of the final image compared to the original container is minimal relative to the size of large language models (LLMs), we reduce the potentially vulnerable packages needed to run in our production environment by excluding these additional resources.</p>
<ul>
<li><strong>Stage 1:</strong> In the first stage of the Containerfile, we will install the <code>huggingface-hub</code> package, copy our script into the container. Then execute it to download the model files from HuggingFace. </li>
<li><strong>Stage 2:</strong> We will copy the model files from the first stage into the second stage.</li>
</ul>
<p>Now let&rsquo;s create a Containerfile with our two-stage build as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plaintext" data-lang="plaintext"><span style="display:flex;"><span>FROM registry.access.redhat.com/ubi9/python-311:latest as base
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>RUN pip install huggingface-hub
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span># Download the model file from hugging face
</span></span><span style="display:flex;"><span>COPY download_model.py .
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>RUN python download_model.py 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span># Final image containing only the essential model files
</span></span><span style="display:flex;"><span>FROM registry.access.redhat.com/ubi9/ubi-micro:9.4
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span># Copy the model files from the base container
</span></span><span style="display:flex;"><span>COPY --from=base /models /models
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>USER 1001
</span></span></code></pre></div><h3 id="step-3-build-the-container">Step 3: Build the container</h3>
<p>To build the container, we will use <code>podman</code> to build the image in our local environment:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plaintext" data-lang="plaintext"><span style="display:flex;"><span>podman build . -t modelcar-example:latest --platform linux/amd64
</span></span></code></pre></div><p>Once the image has been built, you can push the image to a container registry, such as quay.io:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plaintext" data-lang="plaintext"><span style="display:flex;"><span>podman push modelcar-example:latest quay.io/&lt;your-registry&gt;/modelcar-example:latest
</span></span></code></pre></div><p>Our ModelCar image is now ready to be deployed in an OpenShift AI cluster.</p>
<h2 id="deploy-a-model-server-with-a-modelcar-container">Deploy a model server with a ModelCar container</h2>
<p>We will use the vLLM instance that ships with OpenShift AI. To deploy the model from the OpenShift AI dashboard, you must be running OpenShift 2.16 or newer. Your cluster will also require an accelerated computing infrastructure GPU to successfully run the model. An NVIDIA A10 Tensor Core GPU is a great budget option if you are provisioning a new node in your cluster.</p>
<ol>
<li>
<p>To begin, create a new project in the OpenShift AI dashboard where you plan to deploy your model.</p>
</li>
<li>
<p>Next you will need to choose the <strong>Select single-model</strong> option in the Single-model serving platform section.</p>
</li>
<li>
<p>Click the option to <strong>Deploy model</strong> (Figure 1).</p>
<!-- raw HTML omitted -->
<p>
<figure>
  <img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/single-model-serving-platform.png?itok=S4XXUSAJ" alt="Select single model serving" />
</figure>


</p>
<!-- raw HTML omitted -->
<p>Figure 1: Select single model serving.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>On the Deploy model page shown in Figure 2, fill in the following options:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plaintext" data-lang="plaintext"><span style="display:flex;"><span>Model deployment name: granite-3.1-2b-instruct
</span></span><span style="display:flex;"><span>Serving runtime: vLLM ServingRuntime for KServe
</span></span><span style="display:flex;"><span>Number of model server replicas to deploy: 1
</span></span><span style="display:flex;"><span>Model server size: Small
</span></span><span style="display:flex;"><span>Accelerator: nvidia-gpu
</span></span><span style="display:flex;"><span>Number of accelerators: 1
</span></span><span style="display:flex;"><span>Model route: Checked
</span></span><span style="display:flex;"><span>Token authentication: Unchecked 
</span></span></code></pre></div><!-- raw HTML omitted -->
<p>
<figure>
  <img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/deploy-model_0.png?itok=ahKHFlnk" alt="Deploy model form" />
</figure>


</p>
<!-- raw HTML omitted -->
<p>Figure 2: Deploy model form with options filled out.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<ul>
<li>Your accelerator name might be slightly different depending on what your administrator named it when they configured the GPU capabilities in OpenShift AI.</li>
</ul>
</li>
<li>
<p>When you get to the source model location section (Figure 3), choose the <strong>Create connection</strong> option.</p>
</li>
<li>
<p>Set the option type to <strong>UIR - v1</strong>.</p>
</li>
<li>
<p>For the connection details, use the following:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plaintext" data-lang="plaintext"><span style="display:flex;"><span>Connection name: granite-3.1-8b-instruct
</span></span><span style="display:flex;"><span>URI: oci://quay.io/redhat-ai-services/modelcar-catalog:granite-3.1-2b-instruct
</span></span></code></pre></div><!-- raw HTML omitted -->
<p>
<figure>
  <img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/model-source-location.png?itok=_uBHfJa-" alt="Model source location" />
</figure>


</p>
<!-- raw HTML omitted -->
<p>Figure 3: Source model location.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
</li>
<li>
<p>Feel free to replace the image URI with the image you built and published in the previous section.</p>
</li>
<li>
<p>Once everything has been updated, click <strong>Deploy</strong>.</p>
<ul>
<li>A new pod should be created in your namespace to deploy the model. It may take several minutes to successfully pull the image for the first time, but on subsequent startups, the container image should be pre-cached on the node to make the startup much faster.</li>
</ul>
</li>
<li>
<p>By default, KServe creates a KNative Serving object which has a default timeout of 10 minutes. If your model has not successfully deployed in that time, KNative will automatically back the deployment off.  The first time you deploy the LLM on the node it may take longer than 10 minutes to deploy. You can change the default timeout behavior by adding the following annotation to the <code>predictor</code> portion of the <code>InferenceService</code> from the OpenShift AI dashboard:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plaintext" data-lang="plaintext"><span style="display:flex;"><span>apiVersion: serving.kserve.io/v1beta1
</span></span><span style="display:flex;"><span>kind: InferenceService
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span>  name: granite-31-2b-instruct
</span></span><span style="display:flex;"><span>spec:
</span></span><span style="display:flex;"><span>  predictor:
</span></span><span style="display:flex;"><span>    annotations:
</span></span><span style="display:flex;"><span>      serving.knative.dev/progress-deadline: 30m
</span></span></code></pre></div></li>
<li>
<p>After the pod successfully starts, you can now access your LLM by locating the URL in the OpenShift AI dashboard.</p>
</li>
</ol>
<h2 id="modelcar-containers-pros-and-cons">ModelCar containers pros and cons</h2>
<p>ModelCar containers standardize the way an organization delivers models between environments. However, working with ModelCar images can introduce challenges. Let&rsquo;s delve into the pros and cons.</p>
<h4 id="pros">Pros</h4>
<ul>
<li>ModelCar presents a great opportunity to standardize the way in which an organization delivers models between environments by allowing them to leverage the same technologies and automation they likely already have for managing container images. Making the models as portable as any other container image while also reducing the risk of a file not getting copied from one environment to another brings all of the same benefits that we love about container images for application management.</li>
<li>Once cached on a node, the process of starting a vLLM instance can also be significantly faster than the model startup time from S3-compatible storage.</li>
</ul>
<h4 id="cons">Cons</h4>
<ul>
<li>Working with ModelCar images can introduce new challenges due to the size of the LLMs and the corresponding image. A small LLM such as the Granite 2B model creates a container about 5Gb while a medium size model such as the Granite 8B will be about 15Gb. Very large models such as Llama 3.1 405b will be about 900Gb in size, making it very challenging to manage as a container image.</li>
<li>Building large containers can be challenging with the large amount of resources required. Pulling very large container images can quickly overwhelm a node&rsquo;s local cache.</li>
<li>Some of the challenges related to model size may become less pronounced over time as new technologies help bridge the gap. Tools such as KitOps attempt to package models into container images more intelligently, and other improvements are added to Kubernetes to improve pulling large container images.</li>
</ul>
<h2 id="learn-more">Learn more</h2>
<p>In this article, you learned the benefits of ModelCar containers and how to build a ModelCar container image and deploy it with OpenShift AI. To find more patterns and pre-built ModelCar images, take a look at the Red Hat AI Services ModelCar Catalog repo on GitHub and the ModelCar Catalog registry on Quay. </p>
<p>Try Red Hat OpenShift AI and explore what&rsquo;s new.</p>
<p>The post Build and deploy a ModelCar container in OpenShift AI appeared first on Red Hat Developer.</p>
<p>Go to Source</p>

			</div>
		</article>
	</main>
<aside>
	<div>
		<div>
			<h3>LATEST POSTS</h3>
		</div>
		<div>
			<ul>
				
				<li><a href="/posts/2025-03-22-cve-2025-2618---d-link-dap-1620-heap-ba/">CVE-2025-2618 - D-Link DAP-1620 Heap-Based Buffer Overflow Vulnerability</a></li>
				
				<li><a href="/posts/2025-03-22-cve-2025-2619---d-link-dap-1620-stack-b/">CVE-2025-2619 - D-Link DAP-1620 Stack-Based Buffer Overflow Vulnerability</a></li>
				
				<li><a href="/posts/2025-03-22-cve-2025-2186---funnelkit-woocommerce-s/">CVE-2025-2186 - FunnelKit WooCommerce SQL Injection</a></li>
				
				<li><a href="/posts/2025-03-22-cve-2025-2617---yangyouwang-crud-cross-/">CVE-2025-2617 - Yangyouwang Crud Cross-Site Scripting Vulnerability</a></li>
				
				<li><a href="/posts/2025-03-22-cve-2025-26796---apache-oozie-cross-sit/">CVE-2025-26796 - Apache Oozie Cross-site Scripting Vulnerability</a></li>
				
			</ul>
		</div>
	</div>
</aside>


	<footer>
	<p>&copy; 2025 <a href="https://ghariib.ir/"><b>Alireza Gharib. All right reserved</b></a>.
	<a href="https://github.com/Gharib110"><b>Github</b></a>.
	</p>
</footer>

</body>
</html>
