<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<style type=text/css>body{font-family:monospace;}</style>
	<title>NFTables mode for kube-proxy</title>
	
	
	<link rel="stylesheet" href="/css/style.css">
	
	
</head>
<body>
	<header>
	==========================<br>
	== <a href="https://ghariib.ir/">Gharib Personal Blog</a> ==<br>
	==========================
	<div style="float: right;">A Techi Personal Blog</div><br>
	<p>
	<nav>
			<a href="/"><b>Start</b></a>.
			
			
			<a href="/posts/"><b>Posts</b></a>.
			
			<a href="/categories/"><b>Categories</b></a>.
			
			<a href="/tags/"><b>Tags</b></a>.
			
	</nav>
	</p>
	
</header>

	
	<main>
		<article>
			<h1>NFTables mode for kube-proxy</h1>
			<b><time>19.03.2025 00:00</time></b>
		       

			<div>
				<p>A new nftables mode for kube-proxy was introduced as an alpha feature in Kubernetes 1.29. Currently in beta, it is expected to be GA as of 1.33. The new mode fixes long-standing performance problems with the iptables mode and all users running on systems with reasonably-recent kernels are encouraged to try it out. (For compatibility reasons, even once nftables becomes GA, iptables will still be the <em>default</em>.)</p>
<h2 id="why-nftables-part-1-data-plane-latency">Why nftables? Part 1: data plane latency</h2>
<p>The iptables API was designed for implementing simple firewalls, and has problems scaling up to support Service proxying in a large Kubernetes cluster with tens of thousands of Services.</p>
<p>In general, the ruleset generated by kube-proxy in iptables mode has a number of iptables rules proportional to the sum of the number of Services and the total number of endpoints. In particular, at the top level of the ruleset, there is one rule to test each possible Service IP (and port) that a packet might be addressed to:</p>
<pre tabindex="0"><code># If the packet is addressed to 172.30.0.41:80, then jump to the chain
# KUBE-SVC-XPGD46QRK7WJZT7O for further processing
-A KUBE-SERVICES -m comment --comment &#34;namespace1/service1:p80 cluster IP&#34; -m tcp -p tcp -d 172.30.0.41 --dport 80 -j KUBE-SVC-XPGD46QRK7WJZT7O
# If the packet is addressed to 172.30.0.42:443, then...
-A KUBE-SERVICES -m comment --comment &#34;namespace2/service2:p443 cluster IP&#34; -m tcp -p tcp -d 172.30.0.42 --dport 443 -j KUBE-SVC-GNZBNJ2PO5MGZ6GT
# etc...
-A KUBE-SERVICES -m comment --comment &#34;namespace3/service3:p80 cluster IP&#34; -m tcp -p tcp -d 172.30.0.43 --dport 80 -j KUBE-SVC-X27LE4BHSL4DOUIK
</code></pre><p>This means that when a packet comes in, the time it takes the kernel to check it against all of the Service rules is <strong>O(n)</strong> in the number of Services. As the number of Services increases, both the average and the worst-case latency for the first packet of a new connection increases (with the difference between best-case, average, and worst-case being mostly determined by whether a given Service IP address appears earlier or later in the <code>KUBE-SERVICES</code> chain).</p>
<p>
<figure>
  <img src="https://kubernetes.io/blog/2025/02/28/nftables-kube-proxy/iptables-only.svg" alt="kube-proxy iptables first packet latency, at various percentiles, in clusters of various sizes" />
</figure>


</p>
<p>By contrast, with nftables, the normal way to write a ruleset like this is to have a <em>single</em> rule, using a &ldquo;verdict map&rdquo; to do the dispatch:</p>
<pre tabindex="0"><code>table ip kube-proxy {
# The service-ips verdict map indicates the action to take for each matching packet.
map service-ips {
type ipv4_addr . inet_proto . inet_service : verdict
comment &#34;ClusterIP, ExternalIP and LoadBalancer IP traffic&#34;
elements = { 172.30.0.41 . tcp . 80 : goto service-ULMVA6XW-namespace1/service1/tcp/p80,
172.30.0.42 . tcp . 443 : goto service-42NFTM6N-namespace2/service2/tcp/p443,
172.30.0.43 . tcp . 80 : goto service-4AT6LBPK-namespace3/service3/tcp/p80,
... }
}
# Now we just need a single rule to process all packets matching an
# element in the map. (This rule says, &#34;construct a tuple from the
# destination IP address, layer 4 protocol, and destination port; look
# that tuple up in &#34;service-ips&#34;; and if there&#39;s a match, execute the
# associated verdict.)
chain services {
ip daddr . meta l4proto . th dport vmap @service-ips
}
...
}
</code></pre><p>Since there&rsquo;s only a single rule, with a roughly <strong>O(1)</strong> map lookup, packet processing time is more or less constant regardless of cluster size, and the best/average/worst cases are very similar:</p>
<p>
<figure>
  <img src="https://kubernetes.io/blog/2025/02/28/nftables-kube-proxy/nftables-only.svg" alt="kube-proxy nftables first packet latency, at various percentiles, in clusters of various sizes" />
</figure>


</p>
<p>But note the huge difference in the vertical scale between the iptables and nftables graphs! In the clusters with 5000 and 10,000 Services, the p50 (average) latency for nftables is about the same as the p01 (approximately best-case) latency for iptables. In the 30,000 Service cluster, the p99 (approximately worst-case) latency for nftables manages to beat out the p01 latency for iptables by a few microseconds! Here&rsquo;s both sets of data together, but you may have to squint to see the nftables results!:</p>
<p>
<figure>
  <img src="https://kubernetes.io/blog/2025/02/28/nftables-kube-proxy/iptables-vs-nftables.svg" alt="kube-proxy iptables-vs-nftables first packet latency, at various percentiles, in clusters of various sizes" />
</figure>


</p>
<h2 id="why-nftables-part-2-control-plane-latency">Why nftables? Part 2: control plane latency</h2>
<p>While the improvements to data plane latency in large clusters are great, there&rsquo;s another problem with iptables kube-proxy that often keeps users from even being able to grow their clusters to that size: the time it takes kube-proxy to program new iptables rules when Services and their endpoints change.</p>
<p>With both iptables and nftables, the total size of the ruleset as a whole (actual rules, plus associated data) is <strong>O(n)</strong> in the combined number of Services and their endpoints. Originally, the iptables backend would rewrite every rule on every update, and with tens of thousands of Services, this could grow to be hundreds of thousands of iptables rules. Starting in Kubernetes 1.26, we began improving kube-proxy so that it could skip updating <em>most</em> of the unchanged rules in each update, but the limitations of <code>iptables-restore</code> as an API meant that it was still always necessary to send an update that&rsquo;s <strong>O(n)</strong> in the number of Services (though with a noticeably smaller constant than it used to be). Even with those optimizations, it can still be necessary to make use of kube-proxy&rsquo;s <code>minSyncPeriod</code> config option to ensure that it doesn&rsquo;t spend every waking second trying to push iptables updates.</p>
<p>The nftables APIs allow for doing much more incremental updates, and when kube-proxy in nftables mode does an update, the size of the update is only <strong>O(n)</strong> in the number of Services and endpoints that have changed since the last sync, regardless of the total number of Services and endpoints. The fact that the nftables API allows each nftables-using component to have its own private table also means that there is no global lock contention between components like with iptables. As a result, kube-proxy&rsquo;s nftables updates can be done much more efficiently than with iptables.</p>
<p>(Unfortunately I don&rsquo;t have cool graphs for this part.)</p>
<h2 id="why-not-nftables">Why <em>not</em> nftables?</h2>
<p>All that said, there are a few reasons why you might not want to jump right into using the nftables backend for now.</p>
<p>First, the code is still fairly new. While it has plenty of unit tests, performs correctly in our CI system, and has now been used in the real world by multiple users, it has not seen anything close to as much real-world usage as the iptables backend has, so we can&rsquo;t promise that it is as stable and bug-free.</p>
<p>Second, the nftables mode will not work on older Linux distributions; currently it requires a 5.13 or newer kernel. Additionally, because of bugs in early versions of the <code>nft</code> command line tool, you should not run kube-proxy in nftables mode on nodes that have an old (earlier than 1.0.0) version of <code>nft</code> in the host filesystem (or else kube-proxy&rsquo;s use of nftables may interfere with other uses of nftables on the system).</p>
<p>Third, you may have other networking components in your cluster, such as the pod network or NetworkPolicy implementation, that do not yet support kube-proxy in nftables mode. You should consult the documentation (or forums, bug tracker, etc.) for any such components to see if they have problems with nftables mode. (In many cases they will not; as long as they don&rsquo;t try to directly interact with or override kube-proxy&rsquo;s iptables rules, they shouldn&rsquo;t care whether kube-proxy is using iptables or nftables.) Additionally, observability and monitoring tools that have not been updated may report less data for kube-proxy in nftables mode than they do for kube-proxy in iptables mode.</p>
<p>Finally, kube-proxy in nftables mode is intentionally not 100% compatible with kube-proxy in iptables mode. There are a few old kube-proxy features whose default behaviors are less secure, less performant, or less intuitive than we&rsquo;d like, but where we felt that changing the default would be a compatibility break. Since the nftables mode is opt-in, this gave us a chance to fix those bad defaults without breaking users who weren&rsquo;t expecting changes. (In particular, with nftables mode, NodePort Services are now only reachable on their nodes&rsquo; default IPs, as opposed to being reachable on all IPs, including <code>127.0.0.1</code>, with iptables mode.) The kube-proxy documentation has more information about this, including information about metrics you can look at to determine if you are relying on any of the changed functionality, and what configuration options are available to get more backward-compatible behavior.</p>
<h2 id="trying-out-nftables-mode">Trying out nftables mode</h2>
<p>Ready to try it out? In Kubernetes 1.31 and later, you just need to pass <code>--proxy-mode nftables</code> to kube-proxy (or set <code>mode: nftables</code> in your kube-proxy config file).</p>
<p>If you are using kubeadm to set up your cluster, the kubeadm documentation explains how to pass a <code>KubeProxyConfiguration</code> to <code>kubeadm init</code>. You can also deploy nftables-based clusters with <code>kind</code>.</p>
<p>You can also convert existing clusters from iptables (or ipvs) mode to nftables by updating the kube-proxy configuration and restarting the kube-proxy pods. (You do not need to reboot the nodes: when restarting in nftables mode, kube-proxy will delete any existing iptables or ipvs rules, and likewise, if you later revert back to iptables or ipvs mode, it will delete any existing nftables rules.)</p>
<h2 id="future-plans">Future plans</h2>
<p>As mentioned above, while nftables is now the <em>best</em> kube-proxy mode, it is not the <em>default</em>, and we do not yet have a plan for changing that. We will continue to support the iptables mode for a long time.</p>
<p>The future of the IPVS mode of kube-proxy is less certain: its main advantage over iptables was that it was faster, but certain aspects of the IPVS architecture and APIs were awkward for kube-proxy&rsquo;s purposes (for example, the fact that the <code>kube-ipvs0</code> device needs to have <em>every</em> Service IP address assigned to it), and some parts of Kubernetes Service proxying semantics were difficult to implement using IPVS (particularly the fact that some Services had to have different endpoints depending on whether you connected to them from a local or remote client). And now, the nftables mode has the same performance as IPVS mode (actually, slightly better), without any of the downsides:</p>
<p>
<figure>
  <img src="https://kubernetes.io/blog/2025/02/28/nftables-kube-proxy/ipvs-vs-nftables.svg" alt="kube-proxy ipvs-vs-nftables first packet latency, at various percentiles, in clusters of various sizes" />
</figure>


</p>
<p>(In theory the IPVS mode also has the advantage of being able to use various other IPVS functionality, like alternative &ldquo;schedulers&rdquo; for balancing endpoints. In practice, this ended up not being very useful, because kube-proxy runs independently on every node, and the IPVS schedulers on each node had no way of sharing their state with the proxies on other nodes, thus thwarting the effort to balance traffic more cleverly.)</p>
<p>While the Kubernetes project does not have an immediate plan to drop the IPVS backend, it is probably doomed in the long run, and people who are currently using IPVS mode should try out the nftables mode instead (and file bugs if you think there is missing functionality in nftables mode that you can&rsquo;t work around).</p>
<h2 id="learn-more">Learn more</h2>
<ul>
<li>
<p>&ldquo;KEP-3866: Add an nftables-based kube-proxy backend&rdquo; has the history of the new feature.</p>
</li>
<li>
<p>&ldquo;How the Tables Have Turned: Kubernetes Says Goodbye to IPTables&rdquo;, from KubeCon/CloudNativeCon North America 2024, talks about porting kube-proxy and Calico from iptables to nftables.</p>
</li>
<li>
<p>&ldquo;From Observability to Performance&rdquo;, from KubeCon/CloudNativeCon North America 2024. (This is where the kube-proxy latency data came from; the raw data for the charts is also available.)</p>
</li>
</ul>
<p>Go to Source</p>

			</div>
		</article>
	</main>
<aside>
	<div>
		<div>
			<h3>LATEST POSTS</h3>
		</div>
		<div>
			<ul>
				
				<li><a href="/posts/2025-03-20-srsly-risky-biz-chinas-mss-gets-persona/">Srsly Risky Biz Chinas MSS gets personal</a></li>
				
				<li><a href="/posts/2025-03-19-risky-bulletin-google-buys-wiz-for-32-b/">Risky Bulletin Google buys Wiz for 32 billion</a></li>
				
				<li><a href="/posts/apple-inc-sent-you-a-payment-request-payoneer-invoices-other-microsoft-enabled-scams/">“Apple Inc sent you a payment request” Payoneer invoices; other Microsoft-enabled scams</a></li>
				
				<li><a href="/posts/glasses-that-transcribe-text-to-audio/">“Glasses” That Transcribe Text To Audio</a></li>
				
				<li><a href="/posts/5-best-linux-centos-replacement-options-alternatives/">&lt;div&gt;5 Best Linux CentOS Replacement Options &amp; Alternatives&lt;/div&gt;</a></li>
				
			</ul>
		</div>
	</div>
</aside>


	<footer>
	<p>&copy; 2025 <a href="https://ghariib.ir/"><b>Alireza Gharib. All right reserved</b></a>.
	<a href="https://github.com/Gharib110"><b>Github</b></a>.
	</p>
</footer>

</body>
</html>
