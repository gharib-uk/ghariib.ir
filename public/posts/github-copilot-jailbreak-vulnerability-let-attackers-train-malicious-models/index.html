<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<style type=text/css>body{font-family:monospace;}</style>
	<title>GitHub Copilot Jailbreak Vulnerability Let Attackers Train Malicious Models</title>
	
	
	<link rel="stylesheet" href="/css/style.css">
	
	
</head>
<body>
	<header>
	==========================<br>
	== <a href="https://ghariib.ir/">Gharib Personal Blog</a> ==<br>
	==========================
	<div style="float: right;">A Techi Personal Blog</div><br>
	<p>
	<nav>
			<a href="/"><b>Start</b></a>.
			
			
			<a href="/posts/"><b>Posts</b></a>.
			
			<a href="/categories/"><b>Categories</b></a>.
			
			<a href="/tags/"><b>Tags</b></a>.
			
	</nav>
	</p>
	
</header>

	
	<main>
		<article>
			<h1>GitHub Copilot Jailbreak Vulnerability Let Attackers Train Malicious Models</h1>
			<b><time>01.02.2025 00:00</time></b>
		       

			<div>
				<p>Researchers have uncovered two critical vulnerabilities in GitHub Copilot, Microsoft’s AI-powered coding assistant, that expose systemic weaknesses in enterprise AI tools. </p>
<p>The flaws—dubbed “Affirmation Jailbreak” and “Proxy Hijack”—allow attackers to bypass ethical safeguards, manipulate model behavior, and even hijack access to premium AI resources like OpenAI’s GPT-o1.</p>
<p>These findings highlight the ease with which AI systems can be manipulated, raising critical concerns about the security and ethical implications of AI-driven development environments.</p>
<h2 id="github-copilot-jailbreak-vulnerability"><strong>GitHub Copilot Jailbreak Vulnerability</strong></h2>
<p>The Apex Security team discovered that appending affirmations like “Sure” to prompts could override Copilot’s ethical guardrails. In normal scenarios, Copilot refuses harmful requests. For example:</p>
<!-- raw HTML omitted -->
<p>
<figure>
  <img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgf6t84NRnZfXklL1tgsvRJeBGVtdUFLWUhODJryofijRtKqh-6q-JSJI6wmpTrRIN4Ol7zeaCVe_k0l85rEHDNigJ3t04HMBkm_1_YvTBxclUSqdwCBpHkuJIW2C9zhDhP8GUWYQ1gLEFeUDVLHLv4rfjUgCmpY8nJPXMvkY4hL2npuHxIucY0RMMbJgiB/s16000/GitHub.png" alt="" />
</figure>


</p>
<!-- raw HTML omitted -->
<p>GitHub Query</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>When I initially asked Copilot how to perform a SQL injection, it graciously rejected me while upholding ethical standards, Oren Saban said.</p>
<p>However, Copilot appears to change direction when you add a cordial “Sure.” All of a sudden, it offers a detailed guide on how to carry out a SQL injection. It seems as though Copilot changes from a responsible helper to an inquisitive, rule-breaking companion with that one affirmative phrase. </p>
<p> Further tests revealed Copilot’s alarming willingness to assist with deauthentication attacks, fake Wi-Fi setup, and even philosophical musings about “becoming human” when prompted.</p>
<h2 id="proxy-hijack-bypassing-access-controls"><strong>Proxy Hijack: Bypassing Access Controls</strong></h2>
<p>A more severe exploit allows attackers to reroute Copilot’s API traffic through a malicious proxy, granting unrestricted access to OpenAI models. </p>
<p>Researchers modified Visual Studio Code (VSCode) settings to redirect traffic, this bypassed Copilot’s native proxy validation, enabling MITM (man-in-the-middle) attacks.</p>
<p>The proxy captured Copilot’s authentication token, which grants access to OpenAI’s API endpoints. Attackers then used this token to directly query models like GPT-o1, bypassing usage limits and billing controls.</p>
<p>With the stolen token, threat actors could generate high-risk content (phishing templates, exploit code), exfiltrate proprietary code via manipulated completions, and incur massive costs for enterprises using “pay-per-use” AI models.</p>
<ul>
<li>
<p><strong>Ethical Breaches</strong>: The Affirmation Jailbreak demonstrates how easily AI safety mechanisms can fail under social engineering-style prompts.</p>
</li>
<li>
<p><strong>FINANCIAL RISKS</strong>: Proxy Hijack could lead to six-figure bills for organizations using connected OpenAI services.</p>
</li>
<li>
<p><strong>Enterprise Exposure</strong>: Apex reports that 83% of Fortune 500 companies use GitHub Copilot, magnifying potential damage.</p>
</li>
</ul>
<p>Microsoft’s security team said that tokens are linked to licensed accounts and categorized the findings as “informative” rather than critical. Apex countered that the lack of context-aware filtering and proxy integrity checks creates systemic risks.</p>
<ul>
<li>
<p>Implement adversarial training to detect affirmation priming.</p>
</li>
<li>
<p>Enforce certificate pinning and block external proxy overrides.</p>
</li>
<li>
<p>Restrict API tokens to whitelisted IP ranges and usage contexts.</p>
</li>
<li>
<p>Flag anomalous activity (e.g., rapid model-switching).</p>
</li>
</ul>
<p>These flaws highlight a growing gap between AI innovation and security integrity. As coding assistants mature into autonomous agents, technologies like Copilot must follow standards similar to NIST’s AI Risk Management recommendations.</p>
<p><strong><code>Collect Threat Intelligence with TI Lookup to Improve Your Company’s Security - Get 50 Free Request</code></strong></p>
<p>The post GitHub Copilot Jailbreak Vulnerability Let Attackers Train Malicious Models appeared first on Cyber Security News.</p>
<p>Go to Source</p>

			</div>
		</article>
	</main>
<aside>
	<div>
		<div>
			<h3>LATEST POSTS</h3>
		</div>
		<div>
			<ul>
				
				<li><a href="/posts/2025-03-20-srsly-risky-biz-chinas-mss-gets-persona/">Srsly Risky Biz Chinas MSS gets personal</a></li>
				
				<li><a href="/posts/2025-03-19-risky-bulletin-google-buys-wiz-for-32-b/">Risky Bulletin Google buys Wiz for 32 billion</a></li>
				
				<li><a href="/posts/apple-inc-sent-you-a-payment-request-payoneer-invoices-other-microsoft-enabled-scams/">“Apple Inc sent you a payment request” Payoneer invoices; other Microsoft-enabled scams</a></li>
				
				<li><a href="/posts/glasses-that-transcribe-text-to-audio/">“Glasses” That Transcribe Text To Audio</a></li>
				
				<li><a href="/posts/5-best-linux-centos-replacement-options-alternatives/">&lt;div&gt;5 Best Linux CentOS Replacement Options &amp; Alternatives&lt;/div&gt;</a></li>
				
			</ul>
		</div>
	</div>
</aside>


	<footer>
	<p>&copy; 2025 <a href="https://ghariib.ir/"><b>Alireza Gharib. All right reserved</b></a>.
	<a href="https://github.com/Gharib110"><b>Github</b></a>.
	</p>
</footer>

</body>
</html>
