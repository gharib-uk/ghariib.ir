<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta name="generator" content="Hugo 0.145.0">
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<style type=text/css>body{font-family:monospace;}</style>
	<title>Gharib Personal Blog</title>
	
	
	<link rel="stylesheet" href="/css/style.css">
	
	<link rel="alternate" type="application/rss+xml" href="/index.xml" title="Gharib Personal Blog">
</head>
<body>
	<header>
	==========================<br>
	== <a href="https://ghariib.ir/">Gharib Personal Blog</a> ==<br>
	==========================
	<div style="float: right;">A Techi Personal Blog</div><br>
	<p>
	<nav>
			<a href="/"><b>Start</b></a>.
			
			
			<a href="/posts/"><b>Posts</b></a>.
			
			<a href="/categories/"><b>Categories</b></a>.
			
			<a href="/tags/"><b>Tags</b></a>.
			
	</nav>
	</p>
	
</header>

	
	<main>
		
		
			<article>
	<h1><a href="https://ghariib.ir/posts/key-factors-to-evaluate-when-selecting-a-cloud-backup-provider/">Key Factors to Evaluate When Selecting a Cloud Backup Provider</a></h1>
	<b><time>02.01.2025 00:00</time></b>
	
	<div>
		<p>The rise of cloud storage solutions presents companies with numerous options for securing their data, but choosing the right backup provider can be a daunting task. The implications of this choice can affect not only data security but also business continuity.</p>
<p>Selecting a cloud backup provider involves more than just comparing prices; it requires a comprehensive evaluation of various factors that align with your organizationâ€™s unique needs. Key considerations include security measures, integration capabilities, and the terms outlined in service-level agreements. Understanding these elements can help organizations make informed decisions that ultimately safeguard their critical information against unforeseen events.</p>
		
			<a href="https://ghariib.ir/posts/key-factors-to-evaluate-when-selecting-a-cloud-backup-provider/">Read more...</a>
		
	</div>
</article>

		
			<article>
	<h1><a href="https://ghariib.ir/posts/kicking-off-with-a-december-4th-workshop-nist-is-revisiting-and-revising-foundational-cybersecurity-activities-for-iot-device-manufacturers-nist-ir-8259/">Kicking-Off with a December 4th Workshop, NIST is Revisiting and Revising Foundational Cybersecurity Activities for IoT Device Manufacturers, NIST IR 8259!</a></h1>
	<b><time>02.01.2025 00:00</time></b>
	
	<div>
		<p>In May 2020, NIST published Foundational Cybersecurity Activities for IoT Device Manufacturers (NIST IR 8259), which describes recommended cybersecurity activities that manufacturers should consider performing before their IoT devices are sold to customers. These foundational cybersecurity activities can help manufacturers lessen the cybersecurity-related efforts needed by customers, which in turn can reduce the prevalence and severity of IoT device compromises and the attacks performed using compromised devices. In the nearly five years since this document was released, it has been published</p>
		
			<a href="https://ghariib.ir/posts/kicking-off-with-a-december-4th-workshop-nist-is-revisiting-and-revising-foundational-cybersecurity-activities-for-iot-device-manufacturers-nist-ir-8259/">Read more...</a>
		
	</div>
</article>

		
			<article>
	<h1><a href="https://ghariib.ir/posts/kubernetes-1-31-autoconfiguration-for-node-cgroup-driver-beta/">Kubernetes 1.31: Autoconfiguration For Node Cgroup Driver (beta)</a></h1>
	<b><time>02.01.2025 00:00</time></b>
	
	<div>
		<p>Historically, configuring the correct cgroup driver has been a pain point for users running new Kubernetes clusters. On Linux systems, there are two different cgroup drivers: <code>cgroupfs</code> and <code>systemd</code>. In the past, both the kubelet and CRI implementation (like CRI-O or containerd) needed to be configured to use the same cgroup driver, or else the kubelet would exit with an error. This was a source of headaches for many cluster admins. However, there is light at the end of the tunnel!</p>
		
			<a href="https://ghariib.ir/posts/kubernetes-1-31-autoconfiguration-for-node-cgroup-driver-beta/">Read more...</a>
		
	</div>
</article>

		
			<article>
	<h1><a href="https://ghariib.ir/posts/kubernetes-1-31-custom-profiling-in-kubectl-debug-graduates-to-beta/">Kubernetes 1.31: Custom Profiling in Kubectl Debug Graduates to Beta</a></h1>
	<b><time>02.01.2025 00:00</time></b>
	
	<div>
		<p>There are many ways of troubleshooting the pods and nodes in the cluster. However, <code>kubectl debug</code> is one of the easiest, highly used and most prominent ones. It provides a set of static profiles and each profile serves for a different kind of role. For instance, from the network administrator&rsquo;s point of view, debugging the node should be as easy as this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ kubectl debug node/mynode -it --image<span style="color:#f92672">=</span>busybox --profile<span style="color:#f92672">=</span>netadmin
</span></span></code></pre></div><p>On the other hand, static profiles also bring about inherent rigidity, which has some implications for some pods contrary to their ease of use. Because there are various kinds of pods (or nodes) that all have their specific necessities, and unfortunately, some can&rsquo;t be debugged by only using the static profiles.</p>
		
			<a href="https://ghariib.ir/posts/kubernetes-1-31-custom-profiling-in-kubectl-debug-graduates-to-beta/">Read more...</a>
		
	</div>
</article>

		
			<article>
	<h1><a href="https://ghariib.ir/posts/kubernetes-1-31-fine-grained-supplementalgroups-control/">Kubernetes 1.31: Fine-grained SupplementalGroups control</a></h1>
	<b><time>02.01.2025 00:00</time></b>
	
	<div>
		<p>This blog discusses a new feature in Kubernetes 1.31 to improve the handling of supplementary groups in containers within Pods.</p>
<h2 id="motivation-implicit-group-memberships-defined-in-etcgroup-in-the-container-image">Motivation: Implicit group memberships defined in <code>/etc/group</code> in the container image</h2>
<p>Although this behavior may not be popular with many Kubernetes cluster users/admins, kubernetes, by default, <em>merges</em> group information from the Pod with information defined in <code>/etc/group</code> in the container image.</p>
<p>Let&rsquo;s see an example, below Pod specifies <code>runAsUser=1000</code>, <code>runAsGroup=3000</code> and <code>supplementalGroups=4000</code> in the Pod&rsquo;s security context.</p>
		
			<a href="https://ghariib.ir/posts/kubernetes-1-31-fine-grained-supplementalgroups-control/">Read more...</a>
		
	</div>
</article>

		
			<article>
	<h1><a href="https://ghariib.ir/posts/kubernetes-1-31-pod-failure-policy-for-jobs-goes-ga/">Kubernetes 1.31: Pod Failure Policy for Jobs Goes GA</a></h1>
	<b><time>02.01.2025 00:00</time></b>
	
	<div>
		<p>This post describes <em>Pod failure policy</em>, which graduates to stable in Kubernetes 1.31, and how to use it in your Jobs.</p>
<h2 id="about-pod-failure-policy">About Pod failure policy</h2>
<p>When you run workloads on Kubernetes, Pods might fail for a variety of reasons. Ideally, workloads like Jobs should be able to ignore transient, retriable failures and continue running to completion.</p>
<p>To allow for these transient failures, Kubernetes Jobs include the <code>backoffLimit</code> field, which lets you specify a number of Pod failures that you&rsquo;re willing to tolerate during Job execution. However, if you set a large value for the <code>backoffLimit</code> field and rely solely on this field, you might notice unnecessary increases in operating costs as Pods restart excessively until the backoffLimit is met.</p>
		
			<a href="https://ghariib.ir/posts/kubernetes-1-31-pod-failure-policy-for-jobs-goes-ga/">Read more...</a>
		
	</div>
</article>

		
			<article>
	<h1><a href="https://ghariib.ir/posts/kubernetes-1-31-streaming-transitions-from-spdy-to-websockets/">Kubernetes 1.31: Streaming Transitions from SPDY to WebSockets</a></h1>
	<b><time>02.01.2025 00:00</time></b>
	
	<div>
		<p>In Kubernetes 1.31, by default kubectl now uses the WebSocket protocol instead of SPDY for streaming.</p>
<p>This post describes what these changes mean for you and why these streaming APIs matter.</p>
<h2 id="streaming-apis-in-kubernetes">Streaming APIs in Kubernetes</h2>
<p>In Kubernetes, specific endpoints that are exposed as an HTTP or RESTful interface are upgraded to streaming connections, which require a streaming protocol. Unlike HTTP, which is a request-response protocol, a streaming protocol provides a persistent connection that&rsquo;s bi-directional, low-latency, and lets you interact in real-time. Streaming protocols support reading and writing data between your client and the server, in both directions, over the same connection. This type of connection is useful, for example, when you create a shell in a running container from your local workstation and run commands in the container.</p>
		
			<a href="https://ghariib.ir/posts/kubernetes-1-31-streaming-transitions-from-spdy-to-websockets/">Read more...</a>
		
	</div>
</article>

		
			<article>
	<h1><a href="https://ghariib.ir/posts/kubernetes-1-32-moving-volume-group-snapshots-to-beta/">Kubernetes 1.32: Moving Volume Group Snapshots to Beta</a></h1>
	<b><time>02.01.2025 00:00</time></b>
	
	<div>
		<p>Volume group snapshots were introduced as an Alpha feature with the Kubernetes 1.27 release. The recent release of Kubernetes v1.32 moved that support to <strong>beta</strong>. The support for volume group snapshots relies on a set of extension APIs for group snapshots. These APIs allow users to take crash consistent snapshots for a set of volumes. Behind the scenes, Kubernetes uses a label selector to group multiple PersistentVolumeClaims for snapshotting. A key aim is to allow you restore that set of snapshots to new volumes and recover your workload based on a crash consistent recovery point.</p>
		
			<a href="https://ghariib.ir/posts/kubernetes-1-32-moving-volume-group-snapshots-to-beta/">Read more...</a>
		
	</div>
</article>

		
			<article>
	<h1><a href="https://ghariib.ir/posts/kubernetes-v1-31-kubeadm-v1beta4/">Kubernetes v1.31: kubeadm v1beta4</a></h1>
	<b><time>02.01.2025 00:00</time></b>
	
	<div>
		<p>As part of the Kubernetes v1.31 release, <code>kubeadm</code> is adopting a new (v1beta4) version of its configuration file format. Configuration in the previous v1beta3 format is now formally deprecated, which means it&rsquo;s supported but you should migrate to v1beta4 and stop using the deprecated format. Support for v1beta3 configuration will be removed after a minimum of 3 Kubernetes minor releases.</p>
<p>In this article, I&rsquo;ll walk you through key changes; I&rsquo;ll explain about the kubeadm v1beta4 configuration format, and how to migrate from v1beta3 to v1beta4.</p>
		
			<a href="https://ghariib.ir/posts/kubernetes-v1-31-kubeadm-v1beta4/">Read more...</a>
		
	</div>
</article>

		
			<article>
	<h1><a href="https://ghariib.ir/posts/kubernetes-v1-31-new-kubernetes-cpumanager-static-policy-distribute-cpus-across-cores/">Kubernetes v1.31: New Kubernetes CPUManager Static Policy: Distribute CPUs Across Cores</a></h1>
	<b><time>02.01.2025 00:00</time></b>
	
	<div>
		<p>In Kubernetes v1.31, we are excited to introduce a significant enhancement to CPU management capabilities: the <code>distribute-cpus-across-cores</code> option for the CPUManager static policy. This feature is currently in alpha and hidden by default, marking a strategic shift aimed at optimizing CPU utilization and improving system performance across multi-core processors.</p>
<h2 id="understanding-the-feature">Understanding the feature</h2>
<p>Traditionally, Kubernetes&rsquo; CPUManager tends to allocate CPUs as compactly as possible, typically packing them onto the fewest number of physical cores. However, allocation strategy matters, CPUs on the same physical host still share some resources of the physical core, such as the cache and execution units, etc.</p>
		
			<a href="https://ghariib.ir/posts/kubernetes-v1-31-new-kubernetes-cpumanager-static-policy-distribute-cpus-across-cores/">Read more...</a>
		
	</div>
</article>

		
		<div>

	<a href="/page/472/">Previous Page</a>

473 of 553

	<a href="/page/474/">Next Page</a>

</div>

	</main>

	<footer>
	<p>&copy; 2025 <a href="https://ghariib.ir/"><b>Alireza Gharib. All right reserved</b></a>.
	<a href="https://github.com/Gharib110"><b>Github</b></a>.
	</p>
</footer>

</body>
</html>
